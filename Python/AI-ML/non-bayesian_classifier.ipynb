{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "\n",
        "class NBCDiscrete(BaseEstimator, ClassifierMixin):\n",
        "\n",
        "    def __init__(self, domains, laplace=False, logs=False):\n",
        "        self.laplace = laplace\n",
        "        self.logs = logs\n",
        "        self.domains = domains\n",
        "        self.eps = 1e-9\n",
        "    def fit(self, X, y):\n",
        "        self.classes_ = np.unique(y)\n",
        "        K = self.classes_.size # 3\n",
        "        m, n = X.shape # 134, 13\n",
        "        q = np.max(self.domains) # 5\n",
        "        self.PY_ = np.zeros(K) # a priori distribution\n",
        "        self.P_ = np.zeros((K, n, q)) # conditional probs -> P_[2, 7, 3] = Prob(X_7 = 3 | y = 2)\n",
        "\n",
        "        yy = np.zeros(m, dtype=np.int8)\n",
        "        for y_index, label in enumerate(self.classes_):\n",
        "            indexes = y == label\n",
        "            yy[indexes] = y_index\n",
        "            self.PY_[y_index] = np.mean(y == label)\n",
        "        for i in range(m):\n",
        "            for j in range(n):\n",
        "                self.P_[yy[i], j, X[i, j]] += 1\n",
        "        for y_index, label in enumerate(self.classes_):\n",
        "            if not self.laplace:\n",
        "                self.P_[y_index] /= self.PY_[y_index] * m\n",
        "            else:\n",
        "                for j in range(n):\n",
        "                    self.P_[y_index, j] = (self.P_[y_index, j] + 1) / (self.PY_[y_index] * m + self.domains[j])\n",
        "        if self.logs: #ig to jest sposób na trzymanie logów apriori i warunkowych pr.\n",
        "            self.P_LOGS = np.log(self.P_ + self.eps)\n",
        "            self.PY_LOGS = np.log(self.PY_ + self.eps)\n",
        "\n",
        "    def predict(self, X):\n",
        "        return self.classes_[np.argmax(self.predict_proba(X), axis=1)]\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        m, n = X.shape\n",
        "        K = self.classes_.size\n",
        "\n",
        "        if self.logs:\n",
        "            scores = np.zeros((m, K))\n",
        "            for i in range(m):\n",
        "                #apriori\n",
        "                for y_index in range(K):\n",
        "                    scores[i, y_index] += self.PY_LOGS[y_index]\n",
        "                    #warunkowe\n",
        "                    for j in range(n):\n",
        "                        scores[i, y_index] += self.P_LOGS[y_index, j, X[i, j]]\n",
        "\n",
        "\n",
        "        else:\n",
        "            scores = np.ones((m, K))\n",
        "            for i in range(m):\n",
        "                for y_index in range(K):\n",
        "                    for j in range(n):\n",
        "                        scores[i, y_index] *= self.P_[y_index, j, X[i, j]]\n",
        "                    scores *= self.PY_[y_index]\n",
        "            #normalizacja\n",
        "            #mins_ref = np.min(scores, axis=0)\n",
        "            #maxes_ref = np.max(scores, axis=0)\n",
        "            #scores = np.clip(np.floor((scores - mins_ref) / (maxes_ref - mins_ref)), 0, 1)\n",
        "            #musiałem tak zrobić bo dzielenie przez zero mi się pokazywało\n",
        "            score_range = np.max(scores) - np.min(scores)\n",
        "            if score_range == 0:\n",
        "                scores = np.ones_like(scores)\n",
        "            else:\n",
        "                scores = (scores - np.min(scores)) / score_range\n",
        "\n",
        "        return scores"
      ],
      "metadata": {
        "id": "3_3RxoIYW-gI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def read_wine_data(path):\n",
        "    D = np.genfromtxt(path, delimiter=\",\")\n",
        "    X = D[:, 1:]\n",
        "    y = D[:, 0].astype(np.int8)\n",
        "    return X, y\n",
        "\n",
        "def train_test_split(X, y, train_ratio=0.75):\n",
        "    m = X.shape[0]\n",
        "    indexes = np.random.permutation(m)\n",
        "    X = X[indexes]\n",
        "    y = y[indexes]\n",
        "    i = int(np.round(train_ratio * m))\n",
        "    X_train = X[:i]\n",
        "    y_train = y[:i]\n",
        "    X_test = X[i:]\n",
        "    y_test = y[i:]\n",
        "    return X_train, y_train, X_test, y_test\n",
        "\n",
        "def discretize(X, bins, mins_ref=None, maxes_ref=None):\n",
        "    if mins_ref is None:\n",
        "        mins_ref = np.min(X, axis=0)\n",
        "        maxes_ref = np.max(X, axis=0)\n",
        "    X_d = np.clip(np.floor((X - mins_ref) / (maxes_ref - mins_ref) * bins), 0, bins - 1).astype(np.int8)\n",
        "    return X_d, mins_ref, maxes_ref\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    bins = 5 #to jest nasza liczba przedziałów dla zmiennych ciągłych zamienianych na dyskretne\n",
        "    np.random.seed(1)\n",
        "    X, y = read_wine_data(\"/wine.data\") #tu wchodzą nasze dane\n",
        "\n",
        "    X_train, y_train, X_test, y_test = train_test_split(X, y, train_ratio=0.8) #dzielimy na część treningową i testową\n",
        "\n",
        "    X_train_d, mins_ref, maxes_ref = discretize(X_train, bins) #dyskretyzcja\n",
        "    X_test_d, _, _ = discretize(X_test, bins, mins_ref, maxes_ref)\n",
        "\n",
        "    n = X.shape[1]\n",
        "    domains = np.ones(n, dtype=np.int8) * bins # [5, ..., 5] x 13\n",
        "    clf = NBCDiscrete(domains, laplace=True)\n",
        "    clf.fit(X_train_d, y_train)\n",
        "\n",
        "    print(clf.P_)\n",
        "    acc_train = clf.score(X_train_d, y_train)\n",
        "    acc_test = clf.score(X_test_d, y_test)\n",
        "    print(f\"ACC -> TRAIN: {acc_train}, TEST: {acc_test}\")\n",
        "    #znajdujemy zbiór który się podoba, musi spełniać cechy z zmsi, 1000 przykładów 20 atrybutów, zobaczyć które są zmienne które ciągłe, zdyskretyzować tylko te ciągłe, wczytać się gdzie jest atrybut z klasą, dorobić logarytmowanie obliczeń logs=false\n",
        "    #scores if else dla logarytmowania np.log ew trzymać tablicę logów,  chodzi o bezpieczeństwo numeryczne sztuczny test np.tile()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yn51DM9PXDO6",
        "outputId": "06710ff7-ac8c-4c28-bc2f-c52cf3a2ae90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[[0.01818182 0.01818182 0.21818182 0.45454545 0.29090909]\n",
            "  [0.45454545 0.36363636 0.05454545 0.10909091 0.01818182]\n",
            "  [0.01818182 0.01818182 0.21818182 0.47272727 0.27272727]\n",
            "  [0.09090909 0.56363636 0.29090909 0.03636364 0.01818182]\n",
            "  [0.01818182 0.49090909 0.4        0.07272727 0.01818182]\n",
            "  [0.01818182 0.03636364 0.34545455 0.52727273 0.07272727]\n",
            "  [0.01818182 0.01818182 0.14545455 0.52727273 0.29090909]\n",
            "  [0.21818182 0.6        0.12727273 0.03636364 0.01818182]\n",
            "  [0.01818182 0.30909091 0.49090909 0.14545455 0.03636364]\n",
            "  [0.03636364 0.58181818 0.30909091 0.05454545 0.01818182]\n",
            "  [0.01818182 0.05454545 0.47272727 0.41818182 0.03636364]\n",
            "  [0.01818182 0.01818182 0.21818182 0.49090909 0.25454545]\n",
            "  [0.01818182 0.10909091 0.4        0.34545455 0.12727273]]\n",
            "\n",
            " [[0.33870968 0.5        0.11290323 0.03225806 0.01612903]\n",
            "  [0.56451613 0.22580645 0.11290323 0.06451613 0.03225806]\n",
            "  [0.03225806 0.24193548 0.35483871 0.27419355 0.09677419]\n",
            "  [0.03225806 0.27419355 0.5        0.12903226 0.06451613]\n",
            "  [0.53225806 0.30645161 0.06451613 0.0483871  0.0483871 ]\n",
            "  [0.16129032 0.38709677 0.27419355 0.12903226 0.0483871 ]\n",
            "  [0.0483871  0.33870968 0.38709677 0.19354839 0.03225806]\n",
            "  [0.11290323 0.38709677 0.29032258 0.11290323 0.09677419]\n",
            "  [0.14516129 0.4516129  0.24193548 0.11290323 0.0483871 ]\n",
            "  [0.75806452 0.19354839 0.01612903 0.01612903 0.01612903]\n",
            "  [0.01612903 0.20967742 0.35483871 0.27419355 0.14516129]\n",
            "  [0.01612903 0.19354839 0.29032258 0.40322581 0.09677419]\n",
            "  [0.59677419 0.29032258 0.08064516 0.01612903 0.01612903]]\n",
            "\n",
            " [[0.025      0.15       0.45       0.275      0.1       ]\n",
            "  [0.125      0.25       0.325      0.225      0.075     ]\n",
            "  [0.025      0.025      0.225      0.525      0.2       ]\n",
            "  [0.025      0.05       0.625      0.275      0.025     ]\n",
            "  [0.175      0.5        0.275      0.025      0.025     ]\n",
            "  [0.55       0.3        0.075      0.05       0.025     ]\n",
            "  [0.725      0.2        0.025      0.025      0.025     ]\n",
            "  [0.075      0.175      0.25       0.35       0.15      ]\n",
            "  [0.425      0.425      0.075      0.05       0.025     ]\n",
            "  [0.025      0.3        0.2        0.35       0.125     ]\n",
            "  [0.55       0.3        0.1        0.025      0.025     ]\n",
            "  [0.675      0.25       0.025      0.025      0.025     ]\n",
            "  [0.275      0.625      0.05       0.025      0.025     ]]]\n",
            "ACC -> TRAIN: 0.9859154929577465, TEST: 0.9722222222222222\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "#wartości w zbiorze do 10000, wymagany int16\n",
        "def read_student_data(path):\n",
        "    raw_X = np.genfromtxt(path, delimiter=\";\", usecols=range(0, 36))\n",
        "    raw_Y = np.genfromtxt(path, delimiter=\";\", dtype=str, usecols=(-1)) #np.genfromtxt , dtype sobie automatycznie interpretuje kolumny\n",
        "    X = raw_X[1:, :]\n",
        "    y_labels_dict = {\"Dropout\" : 1, \"Enrolled\" : 2, \"Graduate\": 3}\n",
        "    y = np.array([y_labels_dict[value] for value in raw_Y[1:]]).astype(np.int16)\n",
        "    return X, y\n",
        "\n",
        "def train_test_split(X, y, train_ratio=0.75):\n",
        "    m = X.shape[0]\n",
        "    indexes = np.random.permutation(m)\n",
        "    X = X[indexes]\n",
        "    y = y[indexes]\n",
        "    i = int(np.round(train_ratio * m))\n",
        "    X_train = X[:i]\n",
        "    y_train = y[:i]\n",
        "    X_test = X[i:]\n",
        "    y_test = y[i:]\n",
        "    return X_train, y_train, X_test, y_test\n",
        "\n",
        "def discretize(X, bins, mins_ref=None, maxes_ref=None):\n",
        "    if mins_ref is None:\n",
        "        mins_ref = np.min(X, axis=0)\n",
        "        maxes_ref = np.max(X, axis=0)\n",
        "    X_d = np.clip(np.floor((X - mins_ref) / (maxes_ref - mins_ref) * bins), 0, bins - 1).astype(np.int16)\n",
        "    return X_d, mins_ref, maxes_ref\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    bins = [2, 5, 10, 25, 50, 100, 250, 500, 1000]\n",
        "    #np.random.seed(1)\n",
        "\n",
        "    X, y = read_student_data(\"/data.csv\")\n",
        "    print(X.shape)\n",
        "    results = np.empty((len(bins), 3))\n",
        "    X_train, y_train, X_test, y_test = train_test_split(X, y, train_ratio=0.8) #dzielimy na część treningową i testową\n",
        "    for index, bin_value in enumerate(bins):\n",
        "        X_train_d, mins_ref, maxes_ref = discretize(X_train, bin_value) #zmienne ciągłe mają kolumny  [6, 12, 33, 34, 35] jakby co\n",
        "        X_test_d, _, _ = discretize(X_test, bin_value, mins_ref, maxes_ref)\n",
        "\n",
        "        n = X.shape[1]\n",
        "        domains = np.ones(n, dtype=np.int16) * bin_value\n",
        "        clf = NBCDiscrete(domains, laplace=True, logs=True)\n",
        "\n",
        "        clf.fit(X_train_d, y_train)\n",
        "        #print(clf.P_)\n",
        "\n",
        "        acc_train = clf.score(X_train_d, y_train)\n",
        "        acc_test = clf.score(X_test_d, y_test)\n",
        "        results[index] = (bin_value, acc_train, acc_test)\n",
        "        #\n",
        "        print(clf.PY_) #mamy zysk więc to jest korzystny klasyfikator\n",
        "    print(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PCR_rhkcPB28",
        "outputId": "99e23a30-751e-4a74-b0a0-60da401cb8a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(4424, 36)\n",
            "[0.32495055 0.17660356 0.49844589]\n",
            "[0.32495055 0.17660356 0.49844589]\n",
            "[0.32495055 0.17660356 0.49844589]\n",
            "[0.32495055 0.17660356 0.49844589]\n",
            "[0.32495055 0.17660356 0.49844589]\n",
            "[0.32495055 0.17660356 0.49844589]\n",
            "[0.32495055 0.17660356 0.49844589]\n",
            "[0.32495055 0.17660356 0.49844589]\n",
            "[0.32495055 0.17660356 0.49844589]\n",
            "[[2.00000000e+00 6.86634643e-01 6.79096045e-01]\n",
            " [5.00000000e+00 7.37213902e-01 7.10734463e-01]\n",
            " [1.00000000e+01 7.29867194e-01 6.92655367e-01]\n",
            " [2.50000000e+01 7.36931337e-01 6.98305085e-01]\n",
            " [5.00000000e+01 7.50211924e-01 7.12994350e-01]\n",
            " [1.00000000e+02 7.55863238e-01 7.19774011e-01]\n",
            " [2.50000000e+02 7.37213902e-01 7.01694915e-01]\n",
            " [5.00000000e+02 7.21672789e-01 7.02824859e-01]\n",
            " [1.00000000e+03 7.08392201e-01 6.92655367e-01]]\n"
          ]
        }
      ]
    }
  ]
}